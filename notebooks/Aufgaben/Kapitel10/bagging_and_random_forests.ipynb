{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227e9831-a619-44b7-931e-c63c70e22bc5",
   "metadata": {},
   "source": [
    "# Bagging & Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f29d7-79bb-4ef3-8162-c251c81ae61e",
   "metadata": {},
   "source": [
    "1. Nehem wir 10 bootsraped Samples aus einem Datensatz als gegeben an, die \"rote\" und \"grüne\" Klassen als Vorhersagevariable beinhalten. Wir wenden nun 10 Entscheidungsbäume auf den Datensatz $X$ an und erhalten 10 Wahrscheinlichkeiten $P(Klasse ist rot | X)$: \n",
    "\n",
    "$$\n",
    "P(red | X) = \\{0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75 \\} \n",
    "$$\n",
    "\n",
    "    Wenden sie das Majority-Vote an um die finale Klassifizierung zu erhalten.\n",
    "    \n",
    "2. Wir berechnen die Feature Importance für einen Random Forest mit 10 Features ($F_{1} - F_{10}$) als Input. Die Mean-Decrease Impurity (MDI) zeigt für die Features $F_{4}, F_{7}, und F_{9}$ folgende Werte an:\n",
    "\n",
    "    - $F_{4} = 0.33$\n",
    "    - $F_{7} = 0.30$\n",
    "    - $F_{9} = 0.14$\n",
    "    \n",
    "    Alle enderen Features haben MDIs < 0.05\n",
    "    \n",
    "Was bedeutet dies für das Random Forest Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cafe4a8-1d51-40ea-a2b3-2bddaaadbed1",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359cca07-1d37-4c5b-a5be-76e1eb24f841",
   "metadata": {},
   "source": [
    "# Lösungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96954fd3-5daf-4f0d-9dfe-e8384857eda1",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "Wir werden ein Majority Vote durchführen. Da nur 2 Kategorien (rot und grün) als Ausprägungen der Zielvariablen, die es vorherszuagen gilt, existieren, wählen wir als Cutoff-Kriterium 0.5.\n",
    "\n",
    "Zunächst zählen wir also die Vorhersagen > 0.5: \n",
    "\n",
    "$$\n",
    "N_{{y>0.5}} = 6 \n",
    "$$\n",
    "\n",
    "Und dann die Vorhersagen < 0.5\n",
    "\n",
    "$$\n",
    "N_{{y<0.5}} = 4\n",
    "$$\n",
    "\n",
    "Wir sehen, dass 6>4 und somit wählen wir die \"Klasse rot\" als vorhergesagte Klasse.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64833dc-08fa-47e6-b061-3b25ca08e029",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "Dies bedeutete für das Random Forest Model, dass ein Model mit nur diesen 3 Features ausreichend wäre und die anderen Features nicht signifikant zur Verbesserung des Models beitragen. Die anderen Features können somit um Speicher, Trainingszeit nud Vorhersage-Performanz des Models zu sparen aus dem Model entfernt werden.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
