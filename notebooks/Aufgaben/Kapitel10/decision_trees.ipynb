{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74406128-746f-4216-b993-a880140599b7",
   "metadata": {},
   "source": [
    "# Entscheidungsbäume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84373eb-8bac-4740-bd23-53797d36548c",
   "metadata": {},
   "source": [
    "1. Wie wird bei \"greedy\" Aufbau von Entscheidungsbäumen entschieden wann keine Splits mehr durchgeführt werden?\n",
    "\n",
    "2. Was ist \"cost-complexity pruning\" und warum wird es bei Entscheidungsbäumen eingesetzt.\n",
    "\n",
    "3. Nennen Sie die Vor- und Nachteile von Entscheidungsbäumen gegenüber anderen Regressionsverfahren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3320a6ac-da57-42b6-b62b-0f08e95d62a3",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7ba2d-226a-44f7-9781-bac585581fca",
   "metadata": {},
   "source": [
    "## Lösungen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7179d74a-17bc-4d5f-856f-51d540fda05b",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "Beim \"greedy\" Aufbau von Entscheidungsbäumen wir enhand eines Stopp-Kriteriums $K$ enschieden wann keine Splits mehr durchgeführt werden. K umfasst hierbei die Anzahl Datenpunkte in einem Blatt des Baumes. Zum Beispiel kann gestoppt werden, wenn K < 5.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc6bf9c-d8b3-4d21-905b-0f21159ccfda",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "Cost-complexity pruning wird auf einen zuvor vollständig tiefen aufgebauten Entscheidungsbaum angewendet um eine Sequenz an \n",
    "Subbäumen $T$ in Abhängigkeit eines Skalierungsparameters $\\alpha$ zu erhalten. \n",
    "Zu jedem $\\alpha$ gehört also ein Subbaum $T$, sodass gilt:\n",
    "\n",
    " $$\\sum_{m=1}^{\\mid T\\mid} \\sum_{i: x_{i} \\in R_{m}} (y_{i} - \\hat y_{R_{m}})^2 + \\alpha \\mid T \\mid$$,\n",
    " \n",
    " wobei $\\mid T \\mid$ das Minimum der Anzahl Bläter von $T$ darstellt, $R_{m}$ die Region des Prädiktorraums, \n",
    "die zum $m$-ten Blatt gehört, und $y_{R_{m}})$ stellt den vorhergesagten Wert dar, der zur Region $R_{m}$ gehört, \n",
    "also der Mittelwert der Trainingsdaten in dieser Region.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1ab95-903b-4625-b5e8-f58bffb99045",
   "metadata": {},
   "source": [
    "```{toggle}\n",
    "**Vorteile:**\n",
    "\n",
    "- Entscheidungsbäume lassen sich sehr leicht erklären und sind intuitiver als andere Verfahren.\n",
    "- Manche Menschen sind überzeugt, dass Entscheidungsbäume näher an menschlicher Entscheidungsfindung sind als andere Verfahren.\n",
    "- Eintscheidungsbäume können grafisch dargestellt werden und sind leicht zu interpretieren. Dies ermöglicht auch Nicht-Experten einen Zugang zu den damit untersuchten Problemen.\n",
    "- Entscheidungbäume können qualitative Prädiktoren vararbeiten ohne Dummy Variablen einführen zu müssen.\n",
    "\n",
    "**Nachteile:**\n",
    "\n",
    "- In manchen Fällen können Entscheidungsbäume nicht die selbe Vorhersagegenauigkeit aufweisen wie andere Verfahren, innsbesondere bei linearen Zusammenhängen.\n",
    "- Entscheidungsbäume können fragil sein gegenüber kleinen Änderungen in den Daten. Das bedeutet das kleine Änderungen an den Daten relativ große Abweichungen in der Vorhersage produzieren können.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
