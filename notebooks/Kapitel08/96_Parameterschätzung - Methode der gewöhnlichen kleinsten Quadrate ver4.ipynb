{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009c3491-cf87-4f8b-8fad-0e4e9f1ebd53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parameterschätzung - Methode der gewöhnlichen kleinsten Quadrate (OLS)\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23526d98-5c0a-45e1-a703-6073640c545b",
   "metadata": {},
   "source": [
    "Da wir nun die Beschränkungen des deterministischen Modells gelockert und einen Fehlerterm $\\epsilon$ eingeführt haben, stoßen wir auf ein weiteres Problem. Es gibt unendlich viele Regressionsgeraden, die die Spezifikationen des probabilistischen Modells erfüllen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c3401-5c82-4fc3-90a2-92fb0488a39e",
   "metadata": {},
   "source": [
    "![Alt-Text](96_OLS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b5234-177f-4a42-942e-2adeda7ddb56",
   "metadata": {},
   "source": [
    "Offensichtlich brauchen wir eine Strategie, um diejenige Regressionsgerade auszuwählen, die das beste Modell zur Beschreibung der Daten darstellt. In diesem Abschnitt befassen wir uns mit einer der gängigsten Methoden zur Erfüllung dieser Aufgabe, der so genannten Methode der <a href=\"https://en.wikipedia.org/wiki/Ordinary_least_squares\">gewöhnlichen kleinsten Quadrate</a> (englisch ordinary least squares, kurz: $OLS$).\n",
    "\n",
    "Wie im vorigen Abschnitt erwähnt, wird für jedes bestimmte Wertepaar ($x_1,y_1$)\n",
    "wird der Fehler $e_i$ durch $y_1-\\hat y$ berechnet. Um die beste Anpassungsgerade für die gegebenen Daten zu erhalten, wird die **Summe der Fehlerquadrate**, bezeichnet als $SSE$, minimiert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da8ca4-995b-4ca6-a3e9-069bf331ab42",
   "metadata": {},
   "source": [
    "$$min\\; SSE = \\sum_{i=1}^n e_i^2=\\sum_{i=1}^n (y - \\hat y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d9b5fd-0a1f-457d-9593-252377ac0adc",
   "metadata": {},
   "source": [
    "Für das einfache lineare Modell gibt es eine analytische Lösung für $\\beta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c3d529-c0b3-4426-94ee-fc3a5b7ed32e",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta_1} = \\frac{\\sum_{i=1}^n ((x_i- \\bar x) (y_i-\\bar y))}{\\sum_{i=1}^n (x_i-\\bar x)^2} = \\frac{cov(x,y)}{var(x)}\\text{,}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8d7158-cd7d-4148-92cc-5fa4deb86719",
   "metadata": {},
   "source": [
    "und $\\beta_0$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b7533-f51b-4174-9d35-358c36f41a72",
   "metadata": {},
   "source": [
    "$$\\hat{\\beta_0} = \\bar y -\\hat{\\beta_1} \\bar x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fbc67-7c4f-433b-8966-cd20ac6b3123",
   "metadata": {},
   "source": [
    "Die $OLS$ liefert die Maximum-Likelihood-Schätzung für $\\hat \\beta$, wenn die Parameter die gleiche Varianz haben und unkorreliert sind und die Residuen $\\epsilon$ unkorreliert sind und einer Gaußschen Verteilung folgen (<a href=\"https://en.wikipedia.org/wiki/Homoscedasticity\">Homoskedastizität</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4a5d4-7e84-4441-a73d-3237c7c7fa45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
